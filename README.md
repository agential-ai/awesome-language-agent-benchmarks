# Agential-LLM-Agent-Benchmark-List

ðŸ¤— This work is a living document that will be continuously updated to incorporate new insights, address changes, and reflect ongoing developments. Contributions are highly encouraged and can be made through pull requests, issue submissions, or other collaborative means, ensuring the document remains dynamic and evolves with community input.

## ðŸ“– List
---

**General**
 - Shridhar, Mohit, et al. "Alfworld: Aligning text and embodied environments for interactive learning." arXiv preprint arXiv:2010.03768 (2020). [[paper](https://arxiv.org/abs/2010.03768)] [[project](https://alfworld.github.io)]
 - Xie, Jian, et al. "Travelplanner: A benchmark for real-world planning with language agents." arXiv preprint arXiv:2402.01622 (2024). [[paper](https://arxiv.org/abs/2402.01622)] [[project](https://osu-nlp-group.github.io/TravelPlanner/)]
 - Xiao, Ruixuan, et al. "Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents." arXiv preprint arXiv:2406.14884 (2024). [[paper](https://arxiv.org/abs/2406.14884)]
 - Wu, Yue, et al. "Smartplay: A benchmark for llms as intelligent agents." arXiv preprint arXiv:2310.01557 (2023). [[paper](https://arxiv.org/abs/2310.01557)]
 - Khanna, Mukul, et al. "Goat-bench: A benchmark for multi-modal lifelong navigation." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Khanna_GOAT-Bench_A_Benchmark_for_Multi-Modal_Lifelong_Navigation_CVPR_2024_paper.html)] [[project](https://mukulkhanna.github.io/goat-bench/)]
 - Gioacchini, Luca, et al. "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents." arXiv preprint arXiv:2404.06411 (2024). [[paper](https://arxiv.org/abs/2404.06411)] [[project](https://github.com/nec-research/agentquest)]
 - Styles, Olly, et al. "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting." arXiv preprint arXiv:2405.00823 (2024). [[paper](https://arxiv.org/abs/2405.00823)]
 - Liu, Zhiwei, et al. "Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents." arXiv preprint arXiv:2308.05960 (2023). [[paper](https://arxiv.org/abs/2308.05960)] [[project](https://github.com/salesforce/BOLAA)]
 - Siegel, Zachary S., et al. "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark." arXiv preprint arXiv:2409.11363 (2024). [[paper](https://arxiv.org/abs/2409.11363)] [[project](https://github.com/siegelz/core-bench?tab=readme-ov-file)]
 - Yin, Guoli, et al. "MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains." arXiv preprint arXiv:2407.18961 (2024). [[paper](https://arxiv.org/abs/2407.18961)] [[project](https://github.com/apple/axlearn/tree/main/docs/research/mmau)]
 - Wu, Mengsong, et al. "Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark." CCF International Conference on Natural Language Processing and Chinese Computing. Singapore: Springer Nature Singapore, 2024. [[paper](https://link.springer.com/chapter/10.1007/978-981-97-9434-8_29)] [[project](https://github.com/fairyshine/Seal-Tools)]
 - [[paper]()] [[project]()]
- Valmeekam, Karthik, et al. "Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change." Advances in Neural Information Processing Systems 36 (2024). [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/7a92bcdede88c7afd108072faf5485c8-Abstract-Datasets_and_Benchmarks.html)] [[project](https://github.com/karthikv792/LLMs-Planning)]
- Cheng, Ching-An, et al. "Llf-bench: Benchmark for interactive learning from language feedback." arXiv preprint arXiv:2312.06853 (2023). [[paper](https://arxiv.org/abs/2312.06853)] [[project](https://github.com/microsoft/LLF-Bench)]
- Tu, Quan, et al. "Charactereval: A chinese benchmark for role-playing conversational agent evaluation." arXiv preprint arXiv:2401.01275 (2024). [[paper](https://arxiv.org/abs/2401.01275)] [[project](https://github.com/morecry/CharacterEval)]
- Debenedetti, Edoardo, et al. "AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents." arXiv preprint arXiv:2406.13352 (2024). [[paper](https://arxiv.org/abs/2406.13352)] [[project](https://github.com/ethz-spylab/agentdojo)]
- Wu, Cheng-Kuang, et al. "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents." arXiv preprint arXiv:2406.08747 (2024). [[paper](https://arxiv.org/abs/2406.08747)] [[project](https://github.com/stream-bench/stream-bench)]

---
ðŸ’» Computer Use
